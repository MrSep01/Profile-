<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta
      name="description"
      content="Moreland blog post: Reflecting on Chemical Relationships in Our Water - implementing a PBL science assessment."
    />
    <link rel="canonical" href="https://mrsep01.github.io/Profile-/blog-moreland-chemical-relationships-water.html" />
    <meta property="og:type" content="article" />
    <meta property="og:title" content="Reflecting on Chemical Relationships in Our Water | Moreland Blog Posts" />
    <meta property="og:description" content="Moreland blog post: Reflecting on Chemical Relationships in Our Water - implementing a PBL science assessment." />
    <meta property="og:url" content="https://mrsep01.github.io/Profile-/blog-moreland-chemical-relationships-water.html" />
    <meta property="og:image" content="https://mrsep01.github.io/Profile-/assets/sepehr-headshot.png" />
    <meta property="og:site_name" content="Sep Alamouti" />
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Reflecting on Chemical Relationships in Our Water | Moreland Blog Posts" />
    <meta name="twitter:description" content="Moreland blog post: Reflecting on Chemical Relationships in Our Water - implementing a PBL science assessment." />
    <meta name="twitter:image" content="https://mrsep01.github.io/Profile-/assets/sepehr-headshot.png" />
    <title>Reflecting on Chemical Relationships in Our Water | Moreland Blog Posts</title>
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=Red+Hat+Display:wght@400;500;600;700;800&family=Red+Hat+Text:wght@400;500;600;700&display=swap"
      rel="stylesheet"
    />
    <link rel="stylesheet" href="styles.css?v=20260219q" />
    <script src="supabase-config.js?v=20260218j"></script>
    <script src="script.js?v=20260219b" defer></script>
  </head>
  <body>
    <div class="grain" aria-hidden="true"></div>
    <header class="topbar">
      <a class="brand" href="index.html" aria-label="Home">SA</a>
      <nav aria-label="Main">
        <ul class="menu">
          <li><a href="index.html">Home</a></li>
          <li><a href="index.html#contact">Contact</a></li>
          <li class="menu-right-start"><a href="portfolio.html">Portfolio</a></li>
          <li><a class="active" href="blog.html">Blog</a></li>
        </ul>
      </nav>
    </header>

    <section class="page-banner reveal">
      <div class="subpage-banner">
        <div>
          <p class="eyebrow">Moreland Blog Posts</p>
          <h1>Reflecting on Chemical Relationships in Our Water: Implementing a PBL Science Assessment</h1>
          <p class="subtitle">
            Reflection on implementation, assessment design, rubric analysis,
            and future improvements in a Grade 10 chemistry PBL unit.
          </p>
        </div>
        <div class="subpage-banner-media" aria-label="Post banner">
          <p class="subpage-banner-label">Post Banner</p>
          <p class="subpage-banner-note">
            Classroom water-chemistry inquiry and project-based assessment in
            action.
          </p>
        </div>
      </div>
    </section>

    <main>

      <article class="panel reveal blog-post apa-post">
        <p class="blog-card-meta">Moreland Blog Posts</p>

        <h2>Overview of the Project Implementation – Successes and Challenges</h2>
        <p>
          The Chemical Relationships in Our Water project was a project-based learning unit for 10th grade chemistry,
          centered on an authentic local problem of water quality in Bangkok. Students worked in teams to investigate
          water "hardness" and propose affordable treatment solutions, integrating key content on bonding, chemical
          formulas, and stoichiometry. The unit culminated in a public Water Solutions Expo where each team presented
          a design brief with data-driven rationale, supported by a visual product (slides or poster), evidence from a
          lab investigation, stoichiometric calculations, and an oral Q&amp;A. This real-world performance assessment was
          intentionally aligned to InTASC Standard 4 (Content Knowledge) and Standard 5 (Application of Content) – it
          emphasized deep understanding of chemistry concepts and required students to apply that knowledge to solve an
          authentic problem.
        </p>

        <figure class="post-figure">
          <img
            src="assets/chemical-relationships-water-placeholder.svg"
            alt="Students test water samples in a chemistry lab as part of the Chemical Relationships in Our Water project."
          />
          <figcaption>
            Students test water samples in a chemistry lab as part of the "Chemical Relationships in Our Water"
            project, connecting classroom content to real-world water quality issues.
          </figcaption>
        </figure>

        <p>
          Implementing the project revealed several successes.
          <strong>Student Engagement and Content Integration:</strong> Motivation ran high as students realized the
          relevance of chemistry to their community; many were excited to connect ionic reactions to familiar issues
          like kettle scaling and tap-water taste. The variety of learning environments kept things dynamic – we moved
          between classroom planning sessions, a lab investigation (simulating water treatment with safe chemical
          reactions), and even discussing community contexts like local streams and condo water systems. This varied
          approach met science methods expectations by engaging students in different settings (lab and community
          context) and fostering inquiry beyond the classroom. Formative checkpoints were built in (discussed below),
          which helped sustain engagement and ensured content mastery en route to the final expo.
          <strong>Successful Team Outputs:</strong> At the Expo, most teams presented feasible, well-reasoned
          treatment proposals using accurate chemistry. For example, one group designed a simple ion-exchange filter
          for a school lab sink and clearly explained how removing Ca²⁺/Mg²⁺ ions would reduce water hardness. Their
          ability to articulate the link between ionic bonding, precipitate formation, and water quality showed strong
          content understanding. Another success was the public presentation format ("Shark Tank" style pitch), which
          added friendly pressure and authenticity that pushed students to polish their communication. Observing
          students confidently field questions about their projects was a proud moment; it indicated growth in both
          their content knowledge and their 21st-century skills (communication, collaboration, problem-solving).
        </p>

        <p>
          That said, challenges emerged in the assessment process.
          <strong>Time Management and Scaffolding:</strong> Coordinating a multi-faceted project with lab work,
          research, and presentations in a limited time frame was challenging. Some teams initially struggled to divide
          tasks and manage time, risking rushed analysis. I addressed this by instituting "sprint" checkpoints (e.g.
          requiring a draft problem statement, data plan, and partial calculations midway) and holding quick teacher
          conferences with each group to keep them on track. Another challenge was conceptual difficulty, especially
          stoichiometry – a few students hit a wall when balancing equations and doing mole calculations for their
          solutions. Despite class practice, applying these math skills to a new context (like predicting how much lime
          is needed to soften water) was intimidating. To support them, I provided just-in-time mini lessons and
          example calculations during work sessions (pausing group work for a 5-minute re-teach when I spotted
          misconceptions). Lastly, ensuring equal collaboration was an ongoing challenge. While most teams worked well,
          in a couple of groups one member tended to dominate the analysis. I responded by assigning roles (e.g.
          Science Lead, Coordinator) and requiring a written reflection on teamwork from each student. This approach,
          along with peer evaluation on collaboration, helped hold students accountable and largely mitigated
          freeloading. Still, learning to truly collaborate is a growth area - one or two teams experienced minor
          conflicts or uneven participation, which we noted for follow-up in their reflections. Overall, the successes
          – high engagement, authentic learning, and solid final products – outweighed the bumps, but these challenges
          provided valuable lessons for refining the project.
        </p>

        <h2>Guiding Students in Self- and Peer Assessment with the Rubric</h2>
        <p>
          A cornerstone of this project was the use of an analytic rubric to guide both teaching and learning from
          start to finish. Students were not just handed the rubric at the end; they were actively involved in using it
          for self- and peer assessments throughout the unit. Early in the project launch, I introduced a
          student-friendly version of the rubric criteria so students could grasp "what good looks like" for this
          complex task. We discussed each criterion in plain language - for example, translating "Uses balanced
          equations and mole ratios to produce accurate, well-explained calculations..." into what that would mean in
          their own project context. Breaking down the expectations in this way set a clear target and, as research
          suggests, helped demystify the components of success for students. This practice aligns with InTASC Standard
          6 (Assessment): it involved learners in understanding assessment criteria and using ongoing feedback, rather
          than surprises at grading time.
        </p>

        <p>
          During the work process, I frequently turned the rubric into a formative tool. Midway through the project,
          teams conducted peer reviews on each other's draft design briefs and slide decks using a simplified
          single-point rubric (essentially the same criteria, but presented as a checklist with space for comments).
          Each team exchanged work with a peer group and gave one "glow" (strength) and one "grow" (area to improve)
          comment per criterion. To scaffold this, we modeled how to give constructive feedback tied to the rubric -
          for instance, a peer comment on the Evidence criterion might be, "Glow: Your data table is clear and supports
          your claim about water hardness. Grow: Explain more about the limitations of your test - what if there were
          other minerals present?". Students also used the rubric to self-assess their own drafts, marking where they
          believed they were Proficient or where they needed work. This peer- and self-assessment cycle had a
          noticeable impact on motivation and achievement. Students became more invested in meeting the criteria once
          they had literally scored someone else's work - a bit of positive peer pressure kicked in. Several commented
          that seeing a classmate's approach gave them ideas to improve their own. I observed teams going back to
          revise calculations and explanations after peer feedback, aiming for that next level on the rubric. This kind
          of iterative improvement is a hallmark of PBL - it was gratifying to see students treating the rubric not as
          a judgment at the end, but as a roadmap for growth. Indeed, PBL literature emphasizes that rubrics should
          "guide the project, support self- and peer-assessment, and ensure that final products are both authentic and
          rigorous". Using the rubric formatively in this way kept students focused on the learning goals and improved
          the quality of their final products. Moreover, it fostered a sense of ownership: the rubric was no longer
          just the teacher's scoring tool, but the students' tool to gauge their own progress. This empowerment boosted
          their confidence and metacognitive skills - they learned to ask themselves, "Did we back up our claims with
          enough evidence? Does our solution address cost and safety constraints?" - questions pulled straight from the
          rubric language.
        </p>

        <p>
          From a motivation standpoint, involving students in assessment increased transparency and reduced anxiety.
          Knowing the exact criteria for success (and having seen examples during peer review) helped demystify the
          expectations. It was heartening to hear one student say, "I usually worry about what the teacher wants, but
          this rubric let me know exactly what to aim for." Another student noted that giving feedback to peers made
          her more critical of her own project in a constructive way - effectively, the process turned them into
          coaches for each other. This peer feedback process also aligns with InTASC 6 by engaging learners in
          assessing themselves and each other, building self-regulation in learning. Overall, guiding students to use
          the rubric at each stage not only clarified expectations but also created a classroom culture of continuous
          improvement.
        </p>

        <h2>Student Performance Analysis: Rubric Criteria – Successes, Struggles, and Possible Causes</h2>
        <p>
          After the culminating Water Solutions Expo, I analyzed student performance across the rubric's six criteria to
          identify patterns of success and struggle. The analytic rubric covered:
        </p>
        <ul>
          <li>1) Chemistry Content: Bonding &amp; Structure</li>
          <li>2) Stoichiometry &amp; Conservation of Mass</li>
          <li>3) Application to Real-World Problem &amp; Engineering Design</li>
          <li>4) Use of Scientific Practices &amp; Evidence</li>
          <li>5) Communication (visual, written, oral)</li>
          <li>6) Collaboration &amp; Professionalism</li>
        </ul>
        <p>
          Each was scored on a four-point scale (1-Beginning to 4-Exceeds) with detailed descriptors. Reviewing the
          results, a few clear patterns emerged:
        </p>
        <ul>
          <li>
            <strong>Strong Content Knowledge and Real-World Application:</strong> Nearly all students performed well on
            Chemistry Content (Bonding &amp; Structure) and Real-World Application &amp; Design criteria. A majority earned
            Proficient (Level 3) or higher on content understanding; they demonstrated solid grasp of ionic vs.
            covalent bonding and could relate particle-level models to observable properties. For instance, many
            correctly explained how the ionic compound causing water hardness (like calcium carbonate) forms deposits,
            linking chemistry to the "scum" on lab glassware. Some even Exceeded Expectations by providing multiple
            relevant examples and a level of depth beyond the syllabus (e.g. one group connected water conductivity
            changes to ion presence). This success can be attributed to the strong emphasis we placed on conceptual
            understanding (addressing InTASC 4) and the fact that students had multiple exposures to these concepts
            through labs and class activities. Similarly, on the Application to Real-World Problem criterion, most
            teams proposed solutions that were feasible and tailored to a local context, earning high marks. They
            generally did a good job considering practical constraints like cost and safety. One team, for example,
            designed a low-cost charcoal filtration system for a street food vendor and explicitly discussed trade-offs
            (cheap but requiring frequent replacement) - meeting the rubric's "well-reasoned, context-aware solution"
            descriptor. The authenticity of the task likely drove this success: students knew their ideas needed to
            make sense in the real world, not just on paper, and many took that seriously. Where a few fell slightly
            short (scoring Developing) on this criterion, it was usually because they missed one factor - for example,
            a solution that worked chemically but ignored cost until the Q&amp;A brought it up. This suggests those teams
            understood the science but hadn't fully considered all real-world constraints, perhaps due to rushing or
            narrower vision.
          </li>
          <li>
            <strong>Challenges in Quantitative Reasoning (Stoichiometry) and Using Evidence:</strong> The toughest
            areas for students proved to be Stoichiometry &amp; Conservation of Mass and Use of Scientific Evidence.
            These two criteria had the most ratings in the Developing (Level 2) range. Many teams did include balanced
            chemical equations and attempted mole calculations to justify their treatment plans, but several made
            calculation errors or provided only partial explanations. For instance, a common issue was incomplete
            stoichiometric reasoning - a team might balance an equation for removing hardness (e.g. using Na2CO3 to
            precipitate out Ca2+) but then miscalculate the amounts, or fail to clearly show how the moles of reactants
            relate to the moles of products in their design. According to the rubric, Proficient performance required
            mostly correct calculations with some explanation of how they support the design, whereas Developing had
            errors or missing steps and a weak link to the design. A number of students landed in that Developing
            category - their calculations had minor mistakes or they didn't explicitly connect the math to the "why" of
            their solution. I believe the reason is twofold: (1) Stoichiometry is inherently challenging for many 10th
            graders (this was their first major application of mole ratios beyond textbook exercises), and (2) the
            added complexity of an open-ended project meant some students were unsure how precise or extensive their
            calculations needed to be. Despite class practice, transferring those skills to a new context under
            self-direction revealed gaps. In future iterations, more guided practice or exemplars for the calculations
            could help (see improvements section below). The Use of Scientific Practices &amp; Evidence was another
            criterion with mixed performance. Students were tasked with conducting a simple lab or simulation to gather
            data for their proposal (for example, testing a water sample's hardness before and after treatment, or
            using a simulation to predict outcomes). The rubric expected them to "clearly explain investigation design,
            use relevant data as evidence, and discuss limitations" for a top score. In practice, only a few groups
            fully met this level. Those that did presented neat data (one group graphed the drop in water hardness
            after their chemical treatment) and could articulate why their experiment's results supported their claims
            and what the experiment couldn't tell them (limitations). These teams earned Proficient or Exceeds marks,
            demonstrating a solid grasp of scientific practice. However, several groups struggled here, ending up in the
            Developing range. Common issues were vague or weak use of evidence - e.g. saying "our filter worked because
            the water looked clearer" without quantitative data, or not describing how they set up their test. Some
            simply referenced general knowledge ("charcoal is known to absorb impurities") instead of evidence from an
            experiment, reflecting a missed opportunity to apply the scientific method. The root cause might be that
            designing and executing an investigation was a newer skill for many. We had done a lab earlier in the unit,
            but having them plan one largely on their own was ambitious. Time constraints also played a role; a few
            groups admitted they ran out of time to do a thorough test or had to rely on simulation data provided by me,
            which they then cited minimally. This points to a need for tighter integration of the experimental piece in
            the timeline (e.g. dedicate a full class to it), and perhaps more coaching on how to incorporate evidence in
            arguments - skills vital for meeting NGSS science practices. It's worth noting that this criterion
            correlates with InTASC 5 (Application of Content) as well, since it required applying scientific inquiry
            skills. Students' uneven performance here suggests we need to strengthen our support for inquiry processes
            in future iterations.
          </li>
          <li>
            <strong>Communication and Collaboration - Generally Positive, with Room to Grow:</strong> Communication was
            a bright spot for many teams. The rubric's Communication criterion looked at clarity, organization, visual
            design, language use, and Q&amp;A responses. Most groups delivered organized and engaging presentations with
            readable visuals. In fact, about half the teams Exceeded Expectations here - they had polished slide decks
            or posters (some even included infographics of the water cycle relevant to their solution), spoke with
            confidence, and fielded audience questions thoughtfully. We had put emphasis on presentation skills
            (including a rehearsal session), and it paid off. One could feel the pride as students assumed the role of
            "expert" explaining their project to teachers and visiting peers. Minor struggles in communication arose
            mainly in the technical accuracy of visuals and handling of questions. A couple of teams had small
            inaccuracies on their slides (like a mislabeled unit on a graph), or they answered Q&amp;A only briefly,
            indicating a bit of nerves or incomplete preparation for impromptu questions. These issues were relatively
            minor - overall, communication scores were high. On Collaboration &amp; Professionalism, the outcomes were
            mixed but mostly positive. The rubric here considered how well teams shared roles, supported each other,
            stayed on task, and reflected on their teamwork. Based on my observations and the students' own reflections,
            most teams functioned respectfully and productively. They divided work reasonably well and handled the
            stress of the project without major conflict - reflecting at least a Proficient level of collaboration. The
            required reflection journals corroborated this: many students wrote about learning to compromise and valuing
            each member's contributions. However, a few teams had uneven participation, which I also noticed during
            work sessions. In one case, two outspoken students essentially took over the experiment while their quieter
            partner stepped back; in another, one student admitted he "let his teammates do most of the presentation"
            due to stage fright. These teams received a Developing score for collaboration, as their reflection or my
            observations indicated one or two members either dominated or faded into the background. The causes here
            vary: personality differences, varying confidence levels, and perhaps insufficient enforcement of roles on
            my part. It's always a challenge to get perfect equitable collaboration, but the experience underscored the
            need to coach teams more on group dynamics. We did assign roles and checkpoints, but next time I might
            incorporate peer evaluation of contribution to quantify this, or do more frequent check-ins on team process.
            In summary, communication skills were a clear strength of the cohort (likely bolstered by the rubric's
            clarity on what a good presentation entails), while true collaboration remains an area to continuously
            cultivate - an insight that will inform my future facilitation of group work.
          </li>
        </ul>

        <h2>Evaluating the Rubric's Effectiveness and Proposing Improvements</h2>
        <p>
          The analytic rubric proved to be a highly effective tool for this project, though there is always room to
          refine it. In terms of capturing intended learning outcomes, the rubric's six criteria aligned tightly with
          our learning objectives and standards - essentially each major goal (content mastery, quantitative reasoning,
          real-world application, inquiry skills, communication, collaboration) had a corresponding criterion. This
          alignment ensured that important dimensions of student learning were all assessed and made visible. For
          example, by having a separate criterion for Scientific Evidence, I could give targeted feedback on that aspect
          (highlighting to some groups that, say, their lack of data weakened their argument) without that issue being
          masked by an overall grade. In general, the analytic nature of the rubric was valuable because it provided
          clear, focused feedback on each component of the project. Research supports this approach: analytic rubrics
          "break the task into components, helping students understand specific dimensions of quality" and allow
          teachers to pinpoint strengths and weaknesses in complex assignments. I found this to be true - for instance,
          one team that scored well in Content but lower in Evidence could immediately see what to improve, rather than
          just getting a single composite score and wondering where they fell short.
        </p>

        <p>
          Another strength of the rubric was its role in structuring feedback and self-assessment throughout the
          project (not just at the end). By using a student-friendly version in formative checkpoints, it became a
          learning tool rather than just an evaluation tool. This contributed to a more transparent and growth-focused
          classroom culture, as described earlier. The rubric also held up well in terms of validity: its criteria were
          derived from reputable standards (NGSS, IGCSE, InTASC), lending content validity to what we measured. It
          captured both the "hard" academic content and "soft" skills like collaboration, reflecting the holistic
          outcomes of the project. In that sense, it met my goal of assessing not only knowledge but also application
          and process skills, which is essential in performance assessments.
        </p>

        <p>
          However, evaluating its effectiveness also surfaces some areas for improvement in the rubric's design and use.
          One consideration is language clarity and student-friendliness. While the rubric was detailed (almost too
          detailed to fit on one page), some Level descriptors were phrased in academic terms that students might not
          grasp without explanation. For example, "clearly explains investigation design and limitations" or "particle
          representations are missing or incorrect" use terminology students needed interpreted. I mitigated this by
          giving them a simplified version, but I realize the wording itself could be made more student-friendly. In
          future, I might involve students in co-creating or rephrasing the descriptors in their own words. This could
          enhance the rubric's usability and perceived fairness (if they fully understand the language, they're more
          likely to trust and use the rubric effectively).
        </p>

        <p>
          Another aspect to examine is the scope and weight of criteria. We had six criteria, all seemingly equally
          weighted (each on a 4-point scale). In practice, some criteria might deserve heavier weight - for instance,
          should Chemistry Content count a bit more than Collaboration or vice versa? This is a question of what we
          value most in the learning outcomes. If the primary goal is mastery of chemistry and application, one could
          argue those should have more impact on the grade. On the other hand, part of our intent was to elevate skills
          like collaboration, so keeping them equal sends a message that those skills matter too. After reflection, I am
          inclined to keep all six but perhaps explicitly communicate any weighting scheme if I choose one.
          Additionally, six criteria made for a comprehensive rubric, but also a complex one. It's a lot for a teacher
          to score live during presentations (I was juggling a clipboard feverishly during the Expo!). Splitting
          attention across six dimensions in real-time was challenging; I had to scribble quick notes for each. In
          future, I might streamline the rubric slightly or use a scoring app to make it more manageable. For example,
          if evidence and content often correlated, one could combine them - although in this project I believe each was
          distinct enough to keep separate. Another tweak could be to condense the scale or descriptors. We used a
          four-point scale; some colleagues suggested a simpler three-level rubric for student projects to reduce
          perceived grading nuance. However, I found four points useful to distinguish truly exceptional work from solid
          proficiency.
        </p>

        <p>
          Rubric validity and reliability were generally solid but can always be improved. To strengthen validity, I
          could solicit a peer review of the rubric - having another science teacher or curriculum coordinator examine
          whether the descriptors truly match the intended standards and if any important outcome is missing. For
          instance, one could ask: did we sufficiently capture creativity or originality of solutions? (Arguably this
          was indirectly in the Application criterion, but not explicit.) If I value that, maybe it should be more
          evident. Reliability (consistency of scoring) could be enhanced by calibrating with a co-grader. While I was
          the sole assessor this time, in future Expos I might invite a colleague or even industry guest to co-score.
          We would need to norm our understanding of the rubric beforehand to ensure consistency. This would not only
          improve reliability but also give students multiple perspectives in feedback.
        </p>

        <p>
          In summary, the rubric was effective in capturing the multifaceted learning outcomes and providing clear
          feedback aligned with those outcomes. It acted as a "through-line" for the project, tying together the
          learning goals, ongoing formative feedback, and final performance. By design, it significantly enabled both
          assessment of learning and for learning. Going forward, specific improvements to its design will focus on
          clarity (student-friendly language), practicality (perhaps digital scoring or slight streamlining), and
          continued alignment to learning goals (adding anything we missed, like explicitly assessing creativity or
          perseverance if desired). With these tweaks, the rubric's validity and effectiveness will be even stronger.
        </p>

        <h2>Improvements for Future Implementation of the Project</h2>
        <p>
          Reflecting on this first run of Chemical Relationships in Our Water, I have identified several concrete
          improvements for future iterations. These changes aim to enhance the project's structure, assessment
          processes, and tools, further aligning with best practices and standards (including InTASC 4, 5, 6) for
          science education. Here are two key improvements (among others) I plan to implement:
        </p>
        <ul>
          <li>
            <strong>1. Integrate a More Authentic Field/Community Experience:</strong> While the project was rooted in
            a local context, next time I want students to engage even more directly with the community aspect of the
            water problem. One idea is to incorporate a field investigation - for example, taking a field trip to a
            local canal, water treatment facility, or simply around campus to collect water samples. Students could
            test actual samples for hardness or contaminants in the lab (rather than relying solely on provided
            "simulated" data) and use those findings in their proposals. This change would bring the "field"
            environment into play alongside the lab, fulfilling the science methods aim of varied learning environments
            (lab, field, community). It would likely boost student engagement and give them a stronger sense of
            real-world impact. Moreover, interacting with community stakeholders - perhaps interviewing a school
            maintenance staff about our water filters, or a local vendor about their water source - could deepen the
            authenticity. This addition aligns with InTASC 5 by further connecting content to real-world practice, and
            it gives students an even richer platform to apply their content knowledge. Of course, logistical planning
            is needed (safety measures for water collection, permissions, etc.), but the educational payoff of students
            doing science in the field is significant. It would reinforce that chemistry isn't just something in
            textbooks or labs - it's out there in their city's water and environment.
          </li>
          <li>
            <strong>2. Strengthen Scaffolding and Formative Support for Difficult Content and Inquiry Skills:</strong>
            Based on this implementation's challenges, a second major improvement is to bolster the support around
            stoichiometry and scientific inquiry throughout the project. Concretely, I plan to introduce a dedicated
            stoichiometry workshop early in the project timeline. This could be a brief but focused session (or a set
            of Formative.com exercises) where students practice the exact type of calculations they'll need for their
            proposals - for instance, calculating how much of a chemical is required to neutralize a certain
            contaminant, and interpreting what that means for conservation of mass. By front-loading this practice
            (with instant feedback), students should feel more confident applying it in their project, preventing the
            confusion some had this time. Additionally, I will provide an exemplar or template for presenting
            calculations in the design brief. Many students weren't sure how detailed to get; showing a model (like a
            sample calculation write-up for a hypothetical solution) would set a clear standard. For the scientific
            evidence/inquiry piece, I will add more structure to how students plan and report their investigations. One
            improvement is using a simple investigation planning template (a form where students must outline their
            hypothesis, variables, procedure, and how they'll use results) which I will vet before they start the lab.
            In the pilot run, I verbally approved plans but a written template will ensure key aspects (like
            identifying limitations or controls) are considered by each team. This acts as a formative assessment in
            itself - I can catch unrealistic or unclear plans early. During the experiment phase, I might also
            implement a station rotation or conference where I check each team's data collection methods for a few
            minutes, to guide them on the spot. To help with using evidence in arguments, we could do a mini-lesson on
            CER (Claim-Evidence-Reasoning framework) just before they finalize their presentations. Having students
            practice writing a sample CER paragraph about their lab results, and then peer-reviewing it, could sharpen
            their ability to connect evidence to claims - directly addressing the weakness we observed. All these
            scaffolds (templates, exemplars, mini-lessons) are formative supports that align with research-based
            assessment for learning strategies. They should improve student performance in the challenging areas, and
            ultimately lead to a more balanced achievement across all rubric criteria.
          </li>
          <li>
            <strong>3. Refine the Peer Assessment Process with Improved Tools:</strong> While the peer review process was
            beneficial, I saw potential to make it more efficient and impactful. In future, I plan to use a more
            structured peer feedback tool, possibly a simple Microsoft Form or shared online form where peers can
            quickly input their "Glow and Grow" comments aligned to each rubric criterion. This would standardize the
            process and ensure every team receives a comparable set of feedback. It also creates a record I can review
            to monitor the quality of peer feedback being given. Additionally, I will train students a bit more in
            advance on how to critique kindly yet specifically, perhaps by sharing examples of effective feedback
            comments and having them practice on a sample piece of work. The goal is to elevate the quality of peer
            assessment so that it's not only validating but also challenging each team to improve. By sharpening their
            feedback skills, we address a component of InTASC 6 (students engaging in assessment) at a deeper level,
            and we cultivate an even stronger culture of critique and revision in the classroom.
          </li>
          <li>
            <strong>4. Leverage Technology for Assessment and Differentiation:</strong> Finally, I intend to make better
            use of tech tools to streamline assessment and support diverse learners. For instance, using a rubric
            grading app or a tablet during the Expo could allow me to input scores and notes more swiftly, even
            providing immediate digital feedback to students. This would prevent the juggling of papers and possibly
            enable sharing rubric results sooner for reflection. For differentiation, I will continue using tools like
            Formative.com for spaced practice (as I did in this iteration) but tailor them more - e.g., setting
            different difficulty levels for stoichiometry practice based on pre-test results, so advanced students can
            be challenged with extension problems while struggling students get fundamental practice. Similarly,
            employing interactive tools like Curipod for quick formative checks (which I did for vocab and "spot the
            mistake" diagnostics) will remain part of the plan, as they worked well to keep students on their toes.
            These tech integrations make the learning and assessment process more adaptive and responsive to student
            needs, satisfying the intent of InTASC 6 to use varied assessment methods and adjust teaching based on
            data.
          </li>
        </ul>

        <p>
          By implementing these improvements - deeper community engagement, stronger scaffolding of hard skills,
          refined peer review, and smarter use of technology - I anticipate the next run of the project will be even
          more effective at achieving its goals. The essence of the project will remain: students applying chemistry
          content knowledge to solve a real problem (InTASC 4 and 5 in action), and demonstrating their learning
          through a rich performance task assessed with multifaceted criteria (InTASC 6). But with these tweaks, the
          learning experience will be richer and more supportive, and the assessment will more validly and reliably
          capture each student's growth.
        </p>

        <h2>Formative Assessment and Varied Learning Environments in Action</h2>
        <p>
          Throughout this project, I made a concerted effort to use formative assessment techniques and to conduct
          learning in varied environments (classroom, lab, and community context), in line with expectations for
          effective science teaching. These approaches were woven into the fabric of the unit to ensure that learning
          was ongoing, monitored, and connected to real-world contexts.
        </p>

        <p>
          <strong>Formative Assessment:</strong> A range of formative assessments were used to guide instruction and
          provide feedback. At the simplest level, each lesson included quick checks - for example, an exit ticket
          where students annotated a particle diagram of a molecule and explained how its structure might relate to
          properties, which gave me insight into their understanding of bonding that day. We also did a diagnostic
          "Spot the Mistake" voting activity on a poorly written problem statement early in the project, which served
          as a formative assessment of their grasp of what makes a good research question. The peer review sessions
          described above were another powerful formative assessment strategy: students received peer feedback and had a
          chance to iterate on their work before it was summatively assessed. I too gave formative feedback through
          teacher micro-conferences - brief one-on-one (or one-on-team) meetings during work time where I'd check on
          progress and ask probing questions. In these conferences I often used a checklist (based on the rubric) to
          quickly note if a team had a testable hypothesis, needed better data, etc., and then I'd share one immediate
          "next step" suggestion with them. Research by Black &amp; Wiliam (1998) emphasizes the value of such
          assessment for learning practices, and I found them invaluable in this project. The evidence lies in the
          improvements between drafts and finals - teams that struggled initially with their problem statement or
          experimental design mostly had much stronger versions by the end, thanks in part to iterative feedback.
          Formative assessment was not an add-on but a continuous thread, ensuring no student could go too far
          off-track without correction and no misunderstanding remained unaddressed. This approach exemplifies InTASC
          6's core: using multiple assessment methods to monitor learner progress and inform teaching.
        </p>

        <p>
          <strong>Varied Learning Environments:</strong> One of the most exciting aspects of this project was that
          learning happened in multiple environments, not just our usual classroom setting. We deliberately alternated
          between the laboratory and classroom, and embedded the project in a community context:
        </p>
        <ul>
          <li>
            In the lab, students conducted a hands-on investigation (e.g., testing a "mock" hard water sample with
            different treatments). This gave them practical experience with chemical reactions - connecting theory to
            tangible results. Students were visibly more engaged when wearing goggles and handling real glassware, and
            they built lab skills alongside content knowledge. The lab was also where some quieter students shone,
            taking leadership in measuring and observing, which sometimes differs from who speaks up in class. It
            diversified who could demonstrate competence.
          </li>
          <li>
            In the regular classroom, we did planning, research, and discussions. This included computer-based work
            (using Teams for collaborative writing of the design brief and slide preparation) and whole-class strategy
            sessions. The classroom environment was important for peer learning - groups frequently consulted each other
            or had impromptu brainstorming sessions as a class (like when generating a list of local water issues to
            tackle, which we did as a quick competition). I often leveraged the classroom's digital board to model
            thinking processes (recall the modeling of problem statement writing we did together).
          </li>
          <li>
            We also considered the community as a learning context. While we didn't physically go out (this time), the
            project scenario itself was local - Bangkok's water issues. Students were encouraged to think of specific
            sites (one group focused on water in a temple canal, another on their condo's rooftop tank) and even talked
            to family or staff to gather anecdotal input. This contextualization meant that learning wasn't abstract;
            it connected to places and people students know. In some cases, students brought in photos of limescale in
            their home kettle or a sample of tap water from home for our lab tests. By bridging home/community and
            school, the project supported culturally relevant learning and showed students that science is happening
            around them. This addresses InTASC 5's call for meaningful application of content - students truly applied
            chemistry concepts to analyze and potentially improve real conditions in their community.
          </li>
        </ul>

        <p>
          Using these varied settings and methods enriched the learning experience. It caters to different learning
          styles and keeps students active. Importantly, it also reflects the multifaceted nature of scientific work -
          sometimes you're in a lab, sometimes in an office meeting, sometimes out in the field. By mirroring that,
          students get a taste of authentic science practice. For my teaching, it was rewarding to see students who
          might not excel in a traditional test excel in a hands-on environment, and vice versa. It reinforced the
          value of diverse assessment strategies: a student who struggled with writing calculations on paper was able to
          demonstrate his understanding orally during the Q&amp;A, and another who was shy in presentation showed her
          strength in meticulously conducting the lab procedure. In future iterations, as mentioned, I want to push
          this even further by possibly incorporating an actual field activity to collect data.
        </p>

        <p>
          <strong>Meeting Science Methods Expectations:</strong> Ultimately, the project as implemented aligns with the
          science methods rubric expectations by designing learning across varied environments (class, lab, and local
          context) and implementing assessments that showed students applying their disciplinary knowledge in real-world
          contexts. The formative assessments ensured that I, as the teacher, had a finger on the pulse of learning and
          could guide students to success (rather than just evaluating them after the fact). The performance task
          itself - a real-world application of chemistry - provided rich evidence of students' ability to transfer
          classroom learning to a practical challenge. Seeing students justify, for instance, why a certain ion-exchange
          method would work for our school's water fountain, using both chemical equations and consideration of user
          needs, was a strong indication that they weren't just memorizing facts; they were using content knowledge in
          context. This is exactly the outcome we hope for in modern science education and is aligned with InTASC 4 and
          5: their understanding went beyond facts to usable knowledge. Furthermore, by involving them in assessing
          their work and others', we cultivated skills of self-regulation and responsibility (InTASC 6) - students
          learned that assessment is not just something done to them, but a process they are active participants in.
        </p>

        <h2>Conclusion</h2>
        <p>
          In conclusion, reflecting on the Chemical Relationships in Our Water project, I am encouraged by the
          successes and enlightened by the challenges. The experience highlighted how a well-designed performance
          assessment, supported by clear criteria and iterative feedback, can drive significant learning and student
          growth. It demonstrated the power of connecting curriculum to real-world problems - content came alive, and
          students rose to the challenge with creativity and seriousness. The rubric and structured assessments
          provided both the instructor and the learners a clear map of expectations, aligning our efforts. While there
          were bumps along the way (there always are in ambitious projects!), each challenge became an opportunity to
          refine my practice. I've outlined specific improvements that I'm confident will make the next iteration even
          more impactful. As a reflective practitioner, this process also strengthened my alignment with professional
          teaching standards: I deepened my strategies for conveying content (InTASC 4), honed ways to engage students
          in applying that content meaningfully (InTASC 5), and expanded my toolkit of assessments for learning
          (InTASC 6). Writing this reflection for my e-portfolio not only documents the journey but also cements the
          lessons learned. I look forward to implementing these insights and continuing to evolve this project so that
          it can serve as a model of rigorous, authentic, and student-centered science learning.
        </p>
      </article>

      <section class="panel reveal article-nav">
        <a class="btn btn-secondary" href="blog-category-moreland.html"
          >Back to Moreland Category</a
        >
      </section>
    </main>

        <footer class="site-footer">
  <div class="footer-inner">
    <div>
      <p class="footer-brand">Sep Alamouti</p>
      <p class="footer-text">Science Educator | Chemistry (IGCSE / A-Level / IB / AP)</p>
      <p class="footer-text">Bangkok, Thailand</p>
    </div>
    <div>
      <p class="footer-heading">Explore</p>
      <div class="footer-links">
        <a class="footer-link" href="index.html">Home</a>
        <a class="footer-link" href="portfolio.html">Portfolio</a>
        <a class="footer-link" href="blog.html">Blog</a>
        <a class="footer-link" href="teaching-activities.html">Teaching Activities</a>
      </div>
    </div>
    <div>
      <p class="footer-heading">Connect</p>
      <div class="social-list">
        <a class="social-icon" href="mailto:sepalamouti@sepalamouti.com" aria-label="Email">
          <svg viewBox="0 0 24 24" aria-hidden="true"><path d="M2 5h20v14H2V5zm2 2v.2l8 5.3 8-5.3V7l-8 5.3L4 7z"/></svg>
          <span>Email</span>
        </a>
        <a class="social-icon" href="https://www.linkedin.com/in/sep-alamouti" target="_blank" rel="noreferrer" aria-label="LinkedIn">
          <svg viewBox="0 0 24 24" aria-hidden="true"><path d="M4.98 3.5C4.98 4.88 3.86 6 2.49 6S0 4.88 0 3.5 1.12 1 2.49 1s2.49 1.12 2.49 2.5zM.5 8h4V23h-4V8zm7 0h3.8v2.05h.05C11.88 8.98 13.3 8 15.4 8 19.5 8 20 10.7 20 14.2V23h-4v-7.7c0-1.84-.03-4.2-2.56-4.2-2.57 0-2.96 2.01-2.96 4.07V23h-4V8z"/></svg>
          <span>LinkedIn</span>
        </a>
        <a class="social-icon" href="https://github.com/mrsep01" target="_blank" rel="noreferrer" aria-label="GitHub">
          <svg viewBox="0 0 24 24" aria-hidden="true"><path d="M12 .5C5.65.5.5 5.65.5 12c0 5.08 3.29 9.39 7.86 10.91.57.1.78-.25.78-.55 0-.27-.01-1.16-.01-2.1-3.2.7-3.88-1.36-3.88-1.36-.52-1.32-1.28-1.67-1.28-1.67-1.05-.72.08-.71.08-.71 1.16.08 1.77 1.19 1.77 1.19 1.03 1.77 2.71 1.26 3.37.96.1-.75.4-1.26.73-1.55-2.56-.29-5.26-1.28-5.26-5.72 0-1.27.45-2.3 1.19-3.12-.12-.29-.52-1.46.11-3.05 0 0 .97-.31 3.18 1.19a11 11 0 0 1 5.8 0c2.2-1.5 3.17-1.19 3.17-1.19.63 1.59.23 2.76.11 3.05.74.82 1.19 1.85 1.19 3.12 0 4.45-2.7 5.42-5.28 5.71.41.36.77 1.06.77 2.14 0 1.55-.01 2.79-.01 3.17 0 .3.21.66.79.55A11.5 11.5 0 0 0 23.5 12C23.5 5.65 18.35.5 12 .5z"/></svg>
          <span>GitHub</span>
        </a>
        <a class="social-icon" href="tel:+660645585241" aria-label="Phone">
          <svg viewBox="0 0 24 24" aria-hidden="true"><path d="M6.62 10.79a15.4 15.4 0 0 0 6.59 6.59l2.2-2.2a1 1 0 0 1 1.01-.24c1.1.37 2.3.57 3.54.57a1 1 0 0 1 1 1V21a1 1 0 0 1-1 1C9.85 22 2 14.15 2 4a1 1 0 0 1 1-1h4.49a1 1 0 0 1 1 1c0 1.24.2 2.44.57 3.54a1 1 0 0 1-.24 1.01l-2.2 2.24z"/></svg>
          <span>Phone</span>
        </a>
        <a class="social-icon" href="https://web.wechat.com/" target="_blank" rel="noreferrer" aria-label="WeChat ID MRSEP2025">
          <img class="brand-icon-image" src="assets/social/wechat.svg" alt="" loading="lazy" decoding="async" />
          <span>WeChat</span>
        </a>
        <a class="social-icon" href="https://line.me/R/ti/p/~seephralamouti" target="_blank" rel="noreferrer" aria-label="LINE ID seephralamouti">
          <img class="brand-icon-image" src="assets/social/line.svg" alt="" loading="lazy" decoding="async" />
          <span>LINE</span>
        </a>
      </div>
    </div>
  </div>
  <div class="footer-meta">&copy; 2026 Sep Alamouti. All rights reserved.</div>
</footer>
  </body>
</html>
